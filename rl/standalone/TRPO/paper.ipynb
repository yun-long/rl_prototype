{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trust Region Policy Optimization\n",
    "\n",
    "**1. Preliminaris**\n",
    "\n",
    "The environment is Markov decision process environment, defined by the tuple $(\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, r, \\rho_0, \\gamma)$. Let $\\pi$ denote a stochastic policy $\\pi \\sim \\mathcal{N}(a|\\theta^T\\phi(s), \\sigma)$, and $\\eta(\\pi)$ denote its **expected discounted reward**, $Q_{\\pi}(s_t, a_t)$ is **state-value function**, the **value function** $V_{\\pi}(s)$, and the **advantage function** $A_{\\pi}$:\n",
    " > * **expected discounted reward**\n",
    "$$\n",
    "\\eta(\\pi) = \\mathbb{E_{s_0, a_0, \\cdots}} \\left[\\sum_{t=0}^{\\infty} \\gamma^t r(s_t) \\right]\n",
    "$$\n",
    "* **state-action value function**\n",
    "$$\n",
    "Q_{\\pi}(s_t, a_t) = \\mathbb{E_{s_{t+1}, a_{t+1}, \\cdots}} \\left[\\sum_{t=0}^{\\infty} \\gamma^l r(s_l) \\right]\n",
    "$$\n",
    "* **state value function**\n",
    "$$\n",
    "V_{\\pi}(s_t) = \\mathbb{E_{a_{t}, s_{t+1}, \\cdots}} \\left[\\sum_{t=0}^{\\infty} \\gamma^l r(s_l) \\right]\n",
    "$$\n",
    "* **advantage function**\n",
    "$$\n",
    "A_{\\pi}(s, a) = Q_{\\pi}(s, a) - V_{\\pi}(s)\n",
    "$$\n",
    "* \n",
    "$$\n",
    "a_t \\sim \\pi(a_t | s_t), s_{t+1} \\sim P(s_{t+1} | s_t, a_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.Objecitve Function**\n",
    "\n",
    "Suppose $\\tilde{\\pi}$ denote new policy, we have:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\eta(\\tilde{\\pi}) &= \\eta(\\pi) + \\mathbb{E_{s_0, a_0, \\cdots \\sim \\tilde{\\pi}}} \\left[\\sum_{t=0}^{\\infty} \\gamma^t A_{\\pi}(s_t, a_t)\\right] \\\\\n",
    "&= \\eta(\\pi) + \\sum_{s} \\rho_{\\tilde{\\pi}}(s) \\sum_{a} \\tilde{\\pi}(a|s) A_{\\pi}(s, a)\n",
    "\\end{align}\n",
    "$$\n",
    "This equation implies that any policy update $\\pi \\rightarrow \\tilde{\\pi}$that\n",
    "has a nonnegative expected advantage at every state $s$,\n",
    "i.e.,\n",
    "$\\sum_a \\tilde{\\pi}(a|s)A_{\\pi}(s, a)\\geq 0$, is guaranteed to increase\n",
    "the policy performance $\\eta$, or leave it constant in the case\n",
    "that the expected advantage is zero everywhere.\n",
    "\n",
    "However, the complex dependency of $\\rho_{\\tilde{\\pi}}(s)$ on $\\tilde{\\pi}$ makes \n",
    "$\\eta(\\tilde{\\pi})$ difficult to optimize directly. Instead, we introduce the followoing local approximization to $\\eta$:\n",
    "\n",
    "$$\n",
    "L_{\\pi}(\\tilde{\\pi}) = \\eta(\\pi) + \\sum_{s} \\rho_{\\pi}(s) \\sum_{a} \\tilde{\\pi}(a|s) A_{\\pi}(s, a)\n",
    "$$\n",
    "\n",
    "> * Note: $L_{\\pi}(\\tilde{\\pi})$ uses the visitation frequency $\\rho{(\\pi)}$ rather than $\\rho(\\tilde{\\pi})$, ignoring changes in state visitation density due to changes in the policy. \n",
    "* The new policy $\\pi_{\\text{new}}$ was defined to be the following **mixture**:\n",
    "$$\n",
    "\\pi_{\\text{new}}(a|s) = (1-\\alpha)\\pi_{\\text{old}}(a|s) + \\alpha \\text{argmax}_{\\pi'}L_{\\text{old}}(\\pi')\n",
    "$$\n",
    "* Lower bound of the objective\n",
    "$$\n",
    "\\eta{(\\pi_{\\text{new}})} \\geq L_{\\pi_{\\text{old}}}(\\pi_{\\text{new}}) - \\frac{2 \\epsilon \\gamma}{(1-\\gamma)^2} \\alpha^2, \\quad \\text{where} \\quad \\epsilon = \\text{max}_s | \\mathbb{E}_{\\alpha \\sim \\pi'(a|s)}\\left[A_{\\pi}(s, a) \\right]|\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Monotonic Imporvement Guarantee for General Stochastic Policies**\n",
    "\n",
    "Total variation divergence:\n",
    "$$\n",
    "D_{TV}(p || q) = \\frac{1}{2} \\sum_i \\left| p_i - q_i \\right|\n",
    "$$\n",
    "\n",
    "Let $D_{TV}^{\\text{max}}(\\pi_{\\text{old}}, \\pi_{\\text{new}})$, the following bound holds:\n",
    "\n",
    "$$\n",
    "\\eta{(\\pi_{\\text{new}})} \\geq L_{\\pi_{\\text{old}}}(\\pi_{\\text{new}}) - \\frac{4\\epsilon \\gamma}{(1-\\gamma)^2}D_{TV}^{\\text{max}}(\\pi_{\\text{old}}, \\pi_{\\text{new}})^2\n",
    "$$\n",
    "\n",
    "Because: $D_{TV}(p||q)^2 \\leq D_{KL}(p||q)$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\eta{(\\pi_{\\text{new}})} &\\geq L_{\\pi_{\\text{old}}}(\\pi_{\\text{new}}) - \\frac{4\\epsilon \\gamma}{(1-\\gamma)^2}D_{TV}^{\\text{max}}(\\pi_{\\text{old}}, \\pi_{\\text{new}})^2 \\\\\n",
    "& \\geq L_{\\pi_{\\text{old}}}(\\pi_{\\text{new}}) - \\frac{4\\epsilon \\gamma}{(1-\\gamma)^2}D_{KL}^{\\text{max}}(\\pi_{\\text{old}}, \\pi_{\\text{new}}) \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The equation above is guaranteed to generate a monotonically improving sequence of policies $\\eta(\\pi_0) \\leq \\eta(\\pi_1) \\leq \\eta(\\pi_2) \\leq \\cdots$, Becuase, let $M_i (\\pi) = L_{\\pi_i}(\\pi) - \\frac{4\\epsilon \\gamma}{(1-\\gamma)^2}D_{KL}^{\\text{max}}(\\pi_{i}, \\pi)$, we have\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\eta(\\pi_{i+1}) & \\geq M_i(\\pi_{i+1}) \\\\\n",
    "\\eta(\\pi_{i}) & = M_i(\\pi_{i}) \\\\\n",
    "\\eta(\\pi_{i+1}) - \\eta(\\pi_{i}) & \\geq M_i(\\pi_{i+1}) - M_i(\\pi_{i}) \\\\\n",
    "\\end{align}\n",
    "$$\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
